%%%%%%%%%%%%%%%%%%%%%%%%%%  ltexpprt_twocolumn.tex  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is ltexpprt_twocolumn.tex, an example file for use with the SIAM LaTeX2E
% Preprint Series macros. It is designed to provide two-column output.
% Please take the time to read the following comments, as they document
% how to use these macros. This file can be composed and printed out for
% use as sample output.

% Any comments or questions regarding these macros should be directed to:
%
%                 Rachel Ginder
%                 SIAM
%                 3600 University City Science Center
%                 Philadelphia, PA 19104-2688
%                 USA
%                 Telephone: (215) 382-9800
%                 Fax: (215) 386-7999
%                 e-mail: rginder@siam.org


% This file is to be used as an example for style only. It should not be read
% for content.

%%%%%%%%%%%%%%% PLEASE NOTE THE FOLLOWING STYLE RESTRICTIONS %%%%%%%%%%%%%%%

%%  1. There are no new tags.  Existing LaTeX tags have been formatted to match
%%     the Preprint series style.
%%
%%  2. Do not change the margins or page size!  Do not change from the default
%%     text font!
%%
%%  3. You must use \cite in the text to mark your reference citations and
%%     \bibitem in the listing of references at the end of your chapter. See
%%     the examples in the following file. If you are using BibTeX, please
%%     supply the bst file with the manuscript file.
%%
%%  4. This macro is set up for two levels of headings (\section and
%%     \subsection). The macro will automatically number the headings for you.
%%
%%  5. No running heads are to be used for this volume.
%%
%%  6. Theorems, Lemmas, Definitions, Equations, etc. are to be double numbered,
%%     indicating the section and the occurrence of that element
%%     within that section. (For example, the first theorem in the second
%%     section would be numbered 2.1. The macro will
%%     automatically do the numbering for you.
%%
%%  7. Figures and Tables must be single-numbered.
%%     Use existing LaTeX tags for these elements.
%%     Numbering will be done automatically.
%%
%%  8. Page numbering is no longer included in this macro.
%%     Pagination will be set by the program committee.
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[twoside,leqno,twocolumn]{article}

% Comment out the line below if using A4 paper size
\usepackage[letterpaper]{geometry}
\usepackage{amssymb, amsmath, accents}
\usepackage{graphicx, caption, subcaption, tabularx}
\usepackage{biblatex}
\addbibresource{daeos.bib}
\usepackage{ltexpprt}
\usepackage{hyperref}

\newcommand{\abs}[1]{\ensuremath{\left|#1\right|}}
\DeclareMathOperator*{\argmin}{\arg\min}
\newcommand{\bbR}{\ensuremath{\mathbb{R}}}
\newcommand{\bbI}{\ensuremath{\mathbb{I}}}
\newcommand{\frS}{\ensuremath{\mathfrak{S}}}
\newcommand\ubar[1]{\underaccent{\bar}{#1}}

%% add to the AMS macros
\newtheorem{assumption}{Assumption}[section]

\begin{document}

%
\newcommand\relatedversion{}
% \renewcommand\relatedversion{\thanks{The full version of the paper can be accessed at \protect\url{https://arxiv.org/abs/1902.09310}}} % Replace URL with link to full paper or comment out this line


%\setcounter{chapter}{2} % If you are doing your chapter as chapter one,
%\setcounter{section}{3} % comment these two lines out.

\title{\Large An Efficient Local Optimizer-Tracking Solver for Differential-Algebriac Equations with Optimization Criteria\relatedversion}
% which affiliation should I put here?
\author{Alexander Fleming\thanks{RWTH Aachen University.}
\and Jens Deussen\thanks{RWTH Aachen University.}}

\date{}

\maketitle

% Copyright Statement
% When submitting your final paper to a SIAM proceedings, it is requested that you include
% the appropriate copyright in the footer of the paper.  The copyright added should be
% consistent with the copyright selected on the copyright form submitted with the paper.
% Please note that "20XX" should be changed to the year of the meeting.

% Default Copyright Statement
% \fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 20XX by SIAM\\
% Unauthorized reproduction of this article is prohibited}}

% Depending on which copyright you agree to when you sign the copyright form, the copyright
% can be changed to one of the following after commenting out the default copyright statement
% above.

\fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 2024\\
Copyright for this paper is retained by authors}}

%\fancyfoot[R]{\scriptsize{Copyright \textcopyright\ 20XX\\
%Copyright retained by principal author's organization}}

%\pagenumbering{arabic}
%\setcounter{page}{1}%Leave this line commented out.

% send help
\begin{abstract} \small\baselineskip=9pt A solver for differential-algebraic equations with embedded optimization criteria (DAEOs) was developed. The new solver relies on the reduction of a DAEO to a sequence of differential inclusions separated by "jump events". Between jump events, the solver relies solely on an integrator, and it explicitly treats jump events when they occur. This preserves the order of convergence of the integrator component without sacrificing performance to perform global optimization at (nearly) every time step.
\end{abstract}


\section{Problem Setting.}
A differential-algebraic equations with embedded optimization criteria (DAEO) is written as an initial-value problem like so:
\begin{equation} \label{eq:daeo-ivp}
\begin{aligned}
	x(t_0) &= x_0\\
	\dot{x}(t, x, y) &= f(x(t), y(t))\\
	\left\{y^k\right\}(t, x) &= \argmin_{y}h(x(t), y)
\end{aligned}
\end{equation}
where $f:\bbR^{n_x}\times\bbR^{n_y}$ describes the differential behavior of the system, and $h:\bbR^{n_x}\times\bbR^{n_y}$ is an objective function to be minimized. At any given time $t>t_0$, the set of $k$ minimizers of $h$ is denoted by $\left\{y^k\right\}$. The notation $\dot{x}$ denotes $\partial x/\partial t$, $\partial_x f$ denotes $\partial f/\partial x$, and $d_x f$ denotes $df/dx$, in the cases where the difference between the partial and total derivatives is relevant. Second derivatives are written similarly, with $\ddot{x}$ for $\partial^2_t x$ and $\partial^2_{yx} f$ for $\partial f/\partial x\partial y$.

At any time $t \geq t_0$, there will be a some $y^\star(t)\in\left\{y_k(t)\right\}$ such that
\begin{equation}
	h(x(t), y^\star(t))\leq h(x(t),y^i(t))\, \forall\,y^i(t)\in\left\{y^k(t)\right\} 
\end{equation}
% TODO get someone to check the terminology here!
This implies that the initial value problem posed in \eqref{eq:daeo-ivp} can be divided into a sequence of initial-value problems if $y^\star(t)$ has a sufficiently well-behaved dependency on $x$ and $t$. 

\begin{assumption}
	\label{assume:events-exist}
	There are never more than two global optimizers at any time $t$, and there is only a finite set of events $t_0 < t_{e_1} < t_{e_2} < \ldots$ where there are exactly two global optimizers, or:
	\begin{equation*}
		\Lambda\left(\argmin_{y\in\left\{y^k\right\}}h(x(t_{e_j}), y)\right) = 2
	\end{equation*}
\end{assumption}
Given 
\begin{equation*}
	\argmin_{y\in\left\{y^k\right\}} h(x(t), y) = \left\{y^1, y^2\right\}
\end{equation*}
this assumption limits the behavior at each $t_{e_j}$ to something computationally feasible: 
\begin{enumerate}
	\item Two local optimizers $y^1$ and $y^2$ are both global optimizers \textit{only} at $t_{e_j}$:
	\begin{equation*}
		\begin{aligned}
			\lim_{t\to t_{e_j}^+} \argmin_{y} h(x(t), y) &= y^1\\
			\lim_{t\to t_{e_j}^-} \argmin_{y} h(x(t), y) &= y^1
		\end{aligned}
	\end{equation*}
	\item $y^1$ is the global optimizer \textit{before} $t_{e_j}$, and $y^2$ is the global optimizer \textit{after} $t_{e_j}$.
	\begin{equation*}
		\begin{aligned}
			\lim_{t\to t_{e_j}^+} \argmin_{y} h(x(t), y) &= y^1\\
			\lim_{t\to t_{e_j}^-} \argmin_{y} h(x(t), y) &= y^2
		\end{aligned}
	\end{equation*}
\end{enumerate}
% TODO maybe it's relevant to explain that integrating f(x, y2) from t_e to t_e results in no difference in x
Only the second case is relevant for the correct solution of DAEOs, since the duration of the "collision" between $y_1$ and $y_2$ is zero. Between each $t_{e_j}$, the DAEO \eqref{eq:daeo-ivp} can be treated as an IVP, with the extra constraint that $\partial_{y} h(x(t), y^\star(t)) = 0$.

\section{Solution Technique.}
To solve this sequence of IVPs generated by the jump events, the solver must 
\begin{enumerate}
	\item Generate the set $\left\{y^k(t)\right\}$ at $t=t_0$ and at user-specified times $t>t_0$ to correct for any integration error that may accumulate.
	\item Detect when the identity of $y^\star$ changes and correct the time step in which it happens (the second case, above).
	\item Solve the sequence of IVPs after $t_0$ without using the global optimizer.
\end{enumerate}

\subsection{Global Optimization with Interval Arithmetic.}
\begin{assumption} 
	\label{assume:twice-lipschitz}
	Both $f$ and $h$ are twice Lipschitz continuous with respect to $(x(t), y(t))$.
\end{assumption}
Optimality conditions for $h(x(t), y)$ follow from \ref{assume:twice-lipschitz}
\begin{equation}
	\label{eq:optimality-conditions}
	\begin{aligned}
		0 &= \partial_yh(x, y)\\
		0 &\prec\partial^2_{y}h(x, y)
	\end{aligned}
\end{equation}
The global optimizer must find all points $y_i$ that satisfy these optimality conditions. Using a suitable algorithmic differentiation (AD) tool and an interval arithmetic library allows for the comparatively inexpensive verification of the optimality conditions. An interval $[y] \in \bbI\bbR^{n_y} = \left[\ubar y_i, \bar y_i\right]$ bounds all possible values for each $y_i\in\bbR$. 

\begin{Definition}
	\label{def:interval-arith}
	\textbf{(Fundamental Theorem of Interval Arithmetic)}
	
	An interval evaluation of a function $\left[\ubar{y},\bar{y}\right] = f(\left[\ubar x, \bar x\right])$ must yield an interval that contains all pointwise evaluations $f(x)\,\forall\,x\in\left[\ubar x, \bar x\right]$.
\end{Definition}
A correct implementation of interval arithmetic need not provide the tightest possible result $\left[\ubar y, \bar y\right]$\cite{hickeyIntervalArithmeticPrinciples2001}. However, the result $[\ubar y, \bar y] = f(\left[\ubar x, \bar x\right])$ \textit{must} contain all possible results of the interval evaluation. If a computed "interval gradient" does not contain zero, none of the values in the input interval $\left[\ubar x, \bar x\right]$ could be a local optimizer. Similarly, if the "interval Hessian" does not satisfy Sylvester's criterion, the input interval cannot contain a local minimizer.
\begin{Definition}
	\label{def:sylvester}
	\textbf{(Sylvester's Criterion)}
	
	A matrix $A\in\bbR^{n\times n}$ is positive-definite if and only if each of the leading principal minors (the upper left blocks of increasing size) of $A$ have positive determinants.
\end{Definition}
Unfortunately, computing the exact bounds of the interval determinant is an NP-hard problem \cite{horacekDeterminantsIntervalMatrices2018}, but there are methods to determine the sign of the interval determinant that are much faster than computing the interval determinant directly.

\begin{algorithm}
	\label{algo:bnb-ia}
	\textbf{(Branch-and-Bound with Interval Arithmetic)}
	
	Process the list of intervals to search for optimizers $\frS = \left\{[y]\right\}$ according to the following rules:
	\begin{enumerate} 
		\item Take the first item $[y]$ from $\frS$.
		\item Evaluate $[\ubar h, \bar h] = h(x, [y])$. If $\bar h$ is less than the current upper bound for $h$, update it. If $\ubar h$ is larger than the current upper bound for $h$, $[y]$ may be discarded.
		\item Evaluate the interval gradient $\partial_y h(x, [y])$. If the result interval does not contain $0$, $[y]$ contains no optimizers and can be discarded.
		\item Test the interval Hessian $\partial^2_y h(x, [y])$ for positive definite-ness.
		\begin{enumerate}
			\item If the interval Hessian is negative definite, $h$ is concave down over the interval $[y]$, and $[y]$ can be discarded.
			\item If the interval Hessian is positive definite, $h$ is concave up over the interval $[y]$, and $[y]$ can be narrowed by any appropriate local optimization method.
			\item If the interval hessian is neither positive- nor negative definite, bisect the interval $[y]$ in each of its dimensions and append the results to $\frS$
		\end{enumerate}
		\item Repeat for all remaining items in $\frS$.
	\end{enumerate}
\end{algorithm}
This branch-and-bound algorithm locates all intervals $\left\{[y]^k\right\}$ that contain minimizers of $h(x, y)$.

\subsection{Time Stepping}
The trapezoidal rule (a Runge-Kutta scheme of order 2) steps from time step $t^n$ to $t^{n+1}$ via the implicit formulation
\begin{equation}
	\label{eq:trap-rule}
	x(t^{n+1}) = x(t^n) + \frac{\Delta t}{2}\left(\begin{aligned}&f\left(x(t^n), y^\star(t^n)\right)\, + \\ &f\left(x(t^{n+1}), y^\star(t^{n+1})\right) \end{aligned}\right)
\end{equation}
If the time step $\Delta t$ is small enough, the integration step for $x$ can be augmented with $n_y * \Lambda\left(\left\{y^k\right\}\right)$ additional equations that force the values $\left\{y^k\right\}$ at time $t^{n+1}$ to satisfy the first-order optimality conditions:
\begin{equation}
	\label{eq:first-order-opt-at-tn}
	\partial_{y_i}h\left(x(t^{n+1}), y^k_i(t^{n+1})\right) = 0
\end{equation}
	
\eqref{eq:trap-rule} and \eqref{eq:first-order-opt-at-tn} can be combined into one system of equations and solved numerically. However, without an explicit treatment of time steps that bracket a jump event, the order of convergence of \textit{any} chosen integrator will be reduced by 1 \cite{mannshardtOnestepMethodsAny1978}.

\subsection{Jump Event Detection}

Fortunately, explicit treatment of the jump events can restore the order of convergence of the chosen integrator.
\begin{Definition}
	\label{def:event-fn}
	\textbf{(The Event Function)}
	Between any two local optimizers $y^1, y^2 \in \left\{y^k\right\}$, define
	\begin{equation*}
		H(x, y^1, y^2) = h(x, y^1) - h(x, y^2)
	\end{equation*}
	with the note that, at any $t_{e_j}$, an event occurs between $y^1$ and $y^2$ if $H\left(x(t_{e_j}), y^1, y^2\right) = 0$.
\end{Definition}
In order to locate an event, the solver must check the identity of the current global optimizer after \textit{each} time step. If it's changed, a root-finding procedure can be used on the event function to find $t_{e_j}$.


\section{Numerical Experiments}
The basic capabilities and peculiarities of the solver were tested using a simple example with a known analytic solution, and a more complex example designed to test the robustness of the event correction procedure. Linear algebra operations were provided by Eigen\cite{guennebaudEigenV32010}, and automatic differentiation capability was provided by \texttt{dco/c++}\cite{leppkesDerivativeCodeOverloading2016}. For these experiments the solver (written in C++), was compiled using "Release" optimization flags.

% TODO better name
\subsection{Performance Testing}
An analytically-solvable DAEO is
\begin{equation}
	\label{eq:the-easy-one}
	\begin{aligned}
		x(0) &= 1\\
		\dot x(t) &= -(2+y^\star(t))x\\
		\left\{y^k(t)\right\} &= \argmin_y h(x,y)\\
		h(x, y) &= (1-y^2)^2 - (x-\frac{1}{2})\sin\left(\frac{\pi y}{2}\right) 
	\end{aligned}
\end{equation}
Here, the function $h(x, y)$ admits two local optimizers $\left\{-1, 1\right\}$, with 
\begin{equation*}
	y^\star = \begin{cases}
		-1 & x<\frac{1}{2}\\
		1 & x \geq \frac{1}{2}
	\end{cases}
\end{equation*}
The solution for the DAEO is then
\begin{equation*}
	x(t) = \begin{cases}
		\exp-3t & t < -\frac{1}{3}\ln\frac{1}{2} \\
		\exp\left[-t + \frac{2}{3}\ln\frac{1}{2}\right] & t \geq -\frac{1}{3}\ln\frac{1}{2}
	\end{cases}
\end{equation*}
The solver should detect exactly one event at $t_{e_1} = -\frac{1}{3}\ln\frac{1}{2}$. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{gfx/simple_ex_event_err.pdf}
	\caption{Event locator error $\abs{t_{e, calculated} - t_{e, exact}}$. The event locator error converges with order $\Delta t^2$ to the user-set tolerance \texttt{NEWTONTOL}, which is used for every root-finding routine in the solver.}
	\label{fig:easy-event-location}u
\end{figure}

The event location procedure converges quickly enough to satisfy the consistency requirements given in \cite{mannshardtOnestepMethodsAny1978}. Consequently, using the event detection and correction procedure will restore the trapezoidal rule to order 2. As implemented, the user-specified parameter \texttt{NEWTON\_TOL} controls the termination criterion for the event-location procedure, which places a strict limit on its accuracy.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{gfx/easy_daeo_convergence.pdf}
	\caption{Convergence rates of the solver with (marked in blue, converges with order 2) and without (marked in orange, converges with order 1). The event detection procedure generates an extra data point at $t={t_e}$ that lies on the boundary between two of the component IVPs in \eqref{eq:the-easy-one}}.
	\label{fig:easy-error-improvement}
\end{figure}

Figure \ref{fig:easy-error-improvement} shows the order-of-convergence increase due to the event location and simulation procedure. The price of recovering the second-order convergence is the addition of . The user may also trigger a re-search of the optimizer space at specific intervals, which incurs significant computational cost, especially if $n_y > 1$.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{gfx/easy_cost.pdf}
	\begin{tabularx}{0.5\textwidth}{| >{\centering\arraybackslash}X | >{\centering\arraybackslash}X | >{\centering\arraybackslash}X | >{\centering\arraybackslash}X |}
		\hline
		$\Delta t$ & Event Tracking & No Event Tracking & Always Optimize \\
		\hline
		1.0    & 14.8061 ms & 3.10413 ms & 3.63199 ms \\
		0.1    & 13.2307 ms & 4.55644 ms & 28.5151 ms \\
		0.01   & 38.7478 ms & 33.3015 ms & 180.009 ms \\
		0.001  & 302.965 ms & 281.083 ms & 1760.02 ms \\
		1.0e-4 & 2595.12 ms & 2500.12 ms & 17400.6 ms \\
		1.0e-5 & 24997.0 ms & 24707.3 ms & 17853.8 ms \\ \hline
	\end{tabularx}
	\caption{Run time of the solver (ms) compared against the time step size. The cost of global optimization at every time step, even in one dimension, quickly becomes prohibitive as $\Delta t$ decreases. There is little difference between the cost of the solver with event correction enabled or disabled, since the solution to \eqref{eq:the-easy-one} only involves one event correction.}
	\label{fig:easy-cost-comparison}
\end{figure}

% TODO Better title.
\subsection{A More Difficult One} The following example was designed to test the robustness of the solver in a situation where local optimizers emerge and vanish. 
\begin{equation}
	\label{eq:complicated-example}
	\begin{aligned}
		x(0) &= 1\\
		\dot x(t) &= y^\star(t)\\
		\left\{y^k(t)\right\} &= \argmin_{y} h(x, y)\\
		h(x, y) &= (x-y)^2 + \sin 5y
	\end{aligned}
\end{equation}


\newpage
\printbibliography
\end{document}

% End of ltexpprt.tex 