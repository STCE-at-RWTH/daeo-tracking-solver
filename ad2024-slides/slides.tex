\documentclass[10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[url=false]{biblatex}
\addbibresource{daeos.bib}
\renewcommand{\bibfont}{\normalfont\tiny}
\usepackage{color}
\usepackage{url}
\usepackage{algorithmic}
\usepackage{amsmath, amssymb, amsxtra, accents}
\usepackage{graphicx, tabularx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}


\usepackage{listings}
\lstset{language=C++,
    basicstyle=\footnotesize,
    numberstyle=\tiny,
    showstringspaces=false,
    numbers=left,
    frame=none,
    commentstyle=\color{purple},
    captionpos=t
  }

\include{stce-beamer-template}  

\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\newcommand\inc{{\small \;\mathrel{+\!\!=}\;}}
\newcommand\dec{{\small \;\mathrel{-\!\!=}\;}}
\newcommand\ERT{{\footnotesize \textsc{Ert}}}
\newcommand\MEM{{\footnotesize \textsc{Mem}}}
\newcommand\OPS{{\footnotesize \textsc{Ops}}}
\newcommand\RSS{{\footnotesize \textsc{Rss}}}
\newcommand{\dollar}{\mbox{\textdollar}}
\newcommand{\pe}{\mathrel{+}=}
\newcommand{\ass}{\mathrel{:}=}
\newcommand{\A}{{\bf a}}
\newcommand{\C}{{\bf c}}
\newcommand{\B}{{\bf b}}
\renewcommand{\r}{{\bf r}}
\newcommand{\X}{{\bf x}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\dx}{\Delta x}
\newcommand{\Y}{{\bf y}}
\newcommand{\Z}{{\bf z}}
\newcommand{\V}{{\bf v}}
\newcommand{\U}{{\bf u}}
\renewcommand{\P}{{\bf p}}
\newcommand{\Kappa}{{\cal K}}
\newcommand{\fma}{{\small {\tt fma}}}

\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator{\card}{\mathrm{card}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\bbI}{\ensuremath{\mathbb{I}}}
\newcommand{\frS}{\ensuremath{\mathfrak{S}}}
\newcommand\ubar[1]{\underaccent{\bar}{#1}}

\begin{document}
\title[AD2024, Chicago, Sep 16-19, 2024]{{\bf An Efficient Local Optimizer-Tracking Solver for Differential-Algebriac Equations with Optimization Criteria}}
\author[]{Alexander Fleming\footnote{\scriptsize fleming@stce.rwth-aachen.de} \and Jens Deussen \and Uwe Naumann}
\institute[STCE]{\underline{S}oftware and \underline{T}ools for \underline{C}omputational \underline{E}ngineering\footnote{\scriptsize https://www.stce.rwth-aachen.de/} \vspace{1mm} \\ RWTH Aachen, Germany \vspace{5mm} \\ 
8th International Conference on Algorithmic Differentiation (AD2024)
}
		
\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}
	\frametitle{Contents}
	\tableofcontents
\end{frame}

\section{Motivation}

\begin{frame}
\frametitle{Motivation}
\vfill
	Consider the following differential equation with an embedded optimization problem (DAEO):
\begin{equation}
	\label{eq:complicated-example}
	\begin{aligned}
		x(0) &= 1\\
		\dot x(t) &= y^\star(t)\\
		\left\{y^k(t)\right\} &= \argmin_{y} h(x, y)\\
		h(x, y) &= (x-y)^2 + \sin 5y
	\end{aligned}
\end{equation}
\vfill
How might we approach solving this equation numerically?
\vfill
\end{frame}

\begin{frame}
	\frametitle{Motivation}
	\begin{center}
		\includegraphics[width=0.9\columnwidth]{../gfx/minima_shifting.pdf}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Motivation}
	The circular dependence of $x(t)$ on the solution to the minimization problem $y(t)$ poses an issue. We could try separating the two problems, and solving the optimization problem between each time step:
	\vfill
	\begin{equation}
		\begin{aligned}
			\label{eq:bad-time-stepping}
			y^{k\star} &= \argmin_y h(x^k, y)\\
			x^{k+1} &= x^{k} + \frac{\Delta t}{2}\left(f(x^k, y^{k\star}) + f(x^{k+1}, y^{k\star})\right)
		\end{aligned}
	\end{equation}
	\vfill
	This approach has two problems:
	\begin{itemize}
		\item Global optimization is, especially for larger problems, is very expensive.
		\item What happens when the global optimizer $y^\star$ changes between two times $t^{k}, t^{k+1}$?
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Notation}
	\begin{minipage}{0.45\columnwidth}
		\begin{itemize}
			\item $\left\{y_i\right\}$ is the set of minimizers of $h(x, y)$
			\item $y^\star$ is the global minimizer of $h$
			\item $\nabla_y f$ is the gradient of $f$ w.r.t. $y$
			\item $\partial_y f$ and $d_y f$ are partial and total derivatives, respectively. 
		\end{itemize}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\columnwidth}
		\begin{itemize}
			\item $\Delta t$ is the time step size.
			\item $x^k$ is the value of $x$ at time step $t^k$
			\item $y^k_i$ is the value of $y_i$ at time step $t^k$
		\end{itemize}
	\end{minipage}
\end{frame}

\begin{frame}
	\frametitle{Motivation}
	\vfill
	Without some way to detect a global optimizer change during a time step, any integration method we choose for $x(t)$ will have its order of convergence \textit{reduced to 1} \supercite{mannshardtOnestepMethodsAny1978}!
	\vfill
\end{frame}

\section{Global Optimization with Interval Arithmetic}

\subsection{Interval Arithmetic}
\begin{frame}
	\frametitle{Interval Arithmetic}
	In order to solve this problem, we'll first need a global optimizer that can find \textit{all} local optimizers of $h(x, y)$.
	\vfill
	\begin{block}{Fundamental Theorem of Interval Arithmetic \supercite{hickeyIntervalArithmeticPrinciples2001}} 
		An interval evaluation of a function $\left[y\right] = \left[\ubar{y},\bar{y}\right] = f(\left[\ubar x, \bar x\right])=f([x])$ must yield an interval that contains all pointwise evaluations $f(x)\,\forall\,x\in\left[\ubar x, \bar x\right]$.
	\end{block}
	In the world of floating-point numbers, \textbf{IEEE 1788-2015} specifies basic interval arithmetic (IA). The Boost interval library\supercite{melquiondBoostIntervalLibrary2022} follows this standard.
	\vfill
	\vfill
	If we had access to \textit{interval gradients}, this process would be much easier...
	\vfill
\end{frame}

\begin{frame}
	\frametitle{Interval Automatic Differentiation}
	Consider some function $f$, and its tangent- and adjoint-mode evaluations:
	\begin{gather*}
		y = f(x) \colon \R^n\mapsto\R^m\\
		y^{(1)} = \nabla_xf(x)\cdot x^{(1)}\\
		x_{(1)} = y_{(1)}\cdot\nabla_xf(x)
	\end{gather*}
	With a suitable AD tool (perhaps \texttt{dco/c++}\supercite{leppkesDerivativeCodeOverloading2016}), we could substitute intervals directly into tangent- and adjoint-mode AD.
	\vfill
	\begin{minipage}{0.42\columnwidth}
		An interval evaluation of the tangent of $f$ must yield a correct interval for $y^{(1)}$:
		\begin{equation*}
			[y^{(1)}] = \nabla_xf([x])\cdot x^{(1)}
		\end{equation*}
	\end{minipage}
	\hfill
	\begin{minipage}{0.42\columnwidth}
	An interval evaluation of the adjoint of $f$ must yield a correct interval for $x_{(1)}$:
	\begin{equation*}
		[x_{(1)}] = y_{(1)}\cdot\nabla_xf([x])
	\end{equation*}
	\end{minipage}
	\vfill
\end{frame}

\begin{frame}
	\frametitle{Optimality Conditions with Intervals}
	Testing if an interval $[\ubar x, \bar x]$ contains a critical point is quite straightforward.
	\begin{block}{First-Order Optimality with Intervals}
		The interval $[x]$ contains a critical point if $0 \in [\nabla_x f_i]$.
	\end{block}
	\vfill
	Second-order optimality is a bit more difficult. We can compute the interval Hessian $[H_xf]$ via AD exactly as we would compute the regular Hessian.
	\begin{block}{Second-Order Optimality with Intervals}
		The interval $[x]$ contains exactly one minimum of $f$ if, for every matrix $A\in\left[H_xf([x])\right]$, $A$ is positive definite.
	\end{block}
\end{frame}

\subsection{An Optimization Algorithm}
\begin{frame}
	\frametitle{An Optimization Algorithm}
	\begin{block}{Branch-and-Act with Interval Arithmetic}
		Process the list of intervals to search for optimizers $\frS = \left\{[y]\right\}$ according to the following rules:
		\begin{enumerate} 
			\item Take the first item $[y]$ from $\frS$.
			\item \textbf{Gradient Test:} Evaluate the interval gradient $\partial_y h(x, [y])$. If the result interval does not contain $0$, $[y]$ contains no optimizers and can be discarded.
			\item \textbf{Hessian Test:} Test the interval Hessian $\partial^2_y h(x, [y])$ for positive definite-ness.
			\begin{enumerate}
				\item If the interval Hessian is negative definite, $h$ is concave down over the interval $[y]$, and $[y]$ can be discarded.
				\item If the interval Hessian is positive definite, $h$ is concave up over the interval $[y]$, and $[y]$ can be narrowed by any appropriate local optimization method.
			\end{enumerate}
			\item \textbf{Branch:} If the interval Hessian is neither positive- nor negative definite, decompose the interval $[y]$ and append the results to $\frS$.
			\item Repeat for all remaining items in $\frS$.
		\end{enumerate}
	\end{block}
\end{frame}
\section{Integrating a DAEO}
\subsection{Event Detection}
\begin{frame}
	\frametitle{Events}
	The reason that the initial integration strategy fails is because of "events".
	\vfill
	An event occurs at some time $\tau \geq t_0$, when there is a $y^i(\tau)\neq y^\star(\tau)\in\left\{y_k(\tau)\right\}$ such that
	\begin{equation}
		h(x(\tau), y^\star(\tau)) = h(x(\tau), y^i(\tau))
	\end{equation}
	\vfill
	Two scenarios can lead to an event:
	\begin{itemize}
		\item The global optimizer shifts from $y^i(t)$ to $y^j(t)$ as the system evolves.
		\item The global optimizer is both $y^i(\tau)$ and $y^j(\tau)$ \textit{only} at $t=\tau$.
	\end{itemize}
	\vfill
\end{frame}

\begin{frame}
	\frametitle{Events}
	We can test if a time step from $t_k$ to $t_{k+1}$ contained an event between any two local optimizers $y^i$ and $y^j$ by testing for a sign change in the \textit{event function}
	\begin{equation}
		H(x^k, y^i, y^j) = h(x^k, y^i) - h(x^k, y^j)
	\end{equation}
	\vfill
	\vfill
	In time steps where this situation occurs, a root-finding procedure can find the time $\tau$ where $H(x^k, y^i, y^j) = 0$, and the time stepping procedure for \eqref{eq:newtonian-daeo} can step from $t_k$ to $\tau$ to $t_{k+1}$.
	\vfill
\end{frame}

\subsection{Local Optimizer Tracking}

\begin{frame}
	\frametitle{Local Optimizer Tracking}
	\begin{block}{Local Optimizer Drift Estimate}
	\begin{equation}
		\label{eq:local-tracking-guess}
		\begin{aligned}
			0&=\partial_{y}h(x, y_i)\\
			0&=d_x\partial_yh(x, y_i)\\
			0&=\partial^2_{yy}h(x, y_i)\cdot\partial_xy_i + \partial^2_{xy}h(x,y_i)\\
			\partial_xy_i &= -\left(\partial^2_{yy}h(x,y_i)\right)^{-1}\partial^2_{xy}h(x, y_i)\\
			\partial_ty_i &= -\left(\partial^2_{yy}h(x,y_i)\right)^{-1}\partial^2_{xy}h(x, y_i)f(x, y_i)
		\end{aligned}
	\end{equation}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Local Optimizer Tracking}
	\begin{block}{An Integrator for DAEOs}
			Time stepping:
		\begin{equation}
			\label{eq:integrator-with-tracking}
			\begin{aligned}
				0 &= x^k - x^{k+1} + \frac{\Delta t}{2}\left(f(x^k, 	y^{\star,n})+f(x^{k+1}, y^{\star,k+1})\right)\\
				0 &= \partial_{y^k_i}h\left(x^{k+1}, y^{k+1}_i\right)
			\end{aligned}
		\end{equation}
		In practice, a suitable guess for $x^{k+1}$ and $y_i^{k+1}$ is
		\begin{equation*}
			\begin{aligned}
				x^{k+1} &= x^k + f(x^k, y^{\star, k})\Delta t\\
				y_i^{k+1} &= y_i^k + \partial_ty_i^k \Delta t
			\end{aligned}
		\end{equation*}
	\end{block}
\end{frame}

\section{Performance Testing}
\begin{frame}
	\frametitle{A Simpler Example}
	\vfill
		\begin{equation*}
			\label{eq:the-easy-one}
			\begin{aligned}
				x(0) &= 1\\
				\dot x(t) &= -(2+y^\star(t))x\\
				\left\{y^k(t)\right\} &= \argmin_y h(x,y)\\
				h(x, y) &= (1-y^2)^2 - (x-\frac{1}{2})\sin\left(\frac{\pi y}{2}\right) 
			\end{aligned}
		\end{equation*}
	\vfill
\end{frame}

\begin{frame}
	\frametitle{A Simpler Example}
	\begin{center}
		\includegraphics[width=0.9\columnwidth]{../gfx/easy_daeo_solution.pdf}
	\end{center}
\end{frame}
\begin{frame}
	\frametitle{A Simpler Example}
	\begin{center}
		\includegraphics[width=0.9\columnwidth]{../gfx/easy_daeo_convergence.pdf}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{A Simpler Example}
	
	\begin{center}
	%	\includegraphics[width=0.5\textwidth]{gfx/easy_cost.pdf}
	\begin{tabularx}{\textwidth}{| >{\centering\arraybackslash}X | >{\raggedleft\arraybackslash}X |  >{\raggedleft\arraybackslash}X | >{\raggedleft\arraybackslash}X |}
		\hline
		$\Delta t$ & No Event Detection & Event Detection & Always Run Global Optimizer \\
		\hline
		2.50e-01 & 3 ms & 15 ms & 23 ms  \\
		2.50e-02 & 14 ms & 20 ms & 83 ms  \\
		2.50e-03 & 122 ms & 126 ms & 793 ms  \\
		2.50e-04 & 1125 ms & 1051 ms & 7593 ms  \\
		2.50e-05 & 11354 ms & 10444 ms & 74874 ms  \\
		2.50e-06 & 107298 ms & 109945 ms & 744696 ms \\\hline
	\end{tabularx}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{The Original Example}
	\begin{equation*}
		\begin{aligned}
			x(0) &= 1\\
			\dot x(t) &= y^\star(t)\\
			\left\{y^k(t)\right\} &= \argmin_{y} h(x, y)\\
			h(x, y) &= (x-y)^2 + \sin 5y
		\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}
	\frametitle{The Original Example}
	\begin{center}
		\includegraphics[width=0.9\columnwidth]{../gfx/hard_daeo_solution.pdf}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{The Original Example}
	\begin{center}
		\includegraphics[width=0.9\columnwidth]{../gfx/hard_daeo_solution_xdot.pdf}
	\end{center}
\end{frame}

\section{Context}
\begin{frame}
	\frametitle{Context \& Related Work}
	\vfill
	The technique developed here could be applied to process engineering problems:
	\begin{itemize}
		\item Real-time solutions to optimal-control problems \cite{plochDirectSingleShooting2022}.
		\item More accurate simulation of flash processes \cite{ritschelAlgorithmGradientbasedDynamic2018}
	\end{itemize}
	\vfill
	Other multi-scale simulation problems could see significant performance improvments:
	\begin{itemize}
		\item Biorefinery simulation \cite{plochMultiscaleDynamicModeling2019}
	\end{itemize}
	\vfill
\end{frame}
\section{Conclusion and Outlook}
\begin{frame}
	\frametitle{Conclusions}
	\vfill
	\begin{itemize}
		\item We have successfully restored the 2nd-order convergence of an implicit Euler scheme.
		\item We avoid expensive global optimizer calls during the integration of a DAEO.
	\end{itemize}
	\vfill
	For all the juicy details...
	\begin{center}
		\tt\href{https://github.com/STCE-at-RWTH/daeo-tracking-solver}{STCE-at-RWTH/daeo-tracking-solver}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Outlook}
	The current state of the project leads to some natural next steps:
	\begin{itemize}
		\item Can we quantify the quality of an estimate for the local optimizer drift?
		\item How should we compute derivatives w.r.t. $y$ near events?
		\item Is it possible to include second-order information in the time stepping for $\left\{y_i\right\}$?
		\item What are the best choices for hyper-parameters (Event detection threshold, estimates for Lipshitz constants)?
	\end{itemize}
	\vfill
	The global optimizer component could also be very much improved.
	\begin{itemize}
		\item How should we perform global optimization under constraints?
		\item Is there a better way to test interval matrices for positive-definiteness?
	\end{itemize}
\end{frame}

\section{References}
\begin{frame}
	\frametitle{References}
	\printbibliography
\end{frame}



\end{document}
