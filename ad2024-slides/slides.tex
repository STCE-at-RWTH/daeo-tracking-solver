\documentclass[10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[url=false]{biblatex}
\addbibresource{daeos.bib}
\usepackage{color}
\usepackage{url}
\usepackage{algorithmic}
\usepackage{amsmath, amssymb, amsxtra, accents}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}


\usepackage{listings}
\lstset{language=C++,
    basicstyle=\footnotesize,
    numberstyle=\tiny,
    showstringspaces=false,
    numbers=left,
    frame=none,
    commentstyle=\color{purple},
    captionpos=t
  }

\include{stce-beamer-template}  

\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\newcommand\inc{{\small \;\mathrel{+\!\!=}\;}}
\newcommand\dec{{\small \;\mathrel{-\!\!=}\;}}
\newcommand\ERT{{\footnotesize \textsc{Ert}}}
\newcommand\MEM{{\footnotesize \textsc{Mem}}}
\newcommand\OPS{{\footnotesize \textsc{Ops}}}
\newcommand\RSS{{\footnotesize \textsc{Rss}}}
\newcommand{\dollar}{\mbox{\textdollar}}
\newcommand{\pe}{\mathrel{+}=}
\newcommand{\ass}{\mathrel{:}=}
\newcommand{\A}{{\bf a}}
\newcommand{\C}{{\bf c}}
\newcommand{\B}{{\bf b}}
\renewcommand{\r}{{\bf r}}
\newcommand{\X}{{\bf x}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\dx}{\Delta x}
\newcommand{\Y}{{\bf y}}
\newcommand{\Z}{{\bf z}}
\newcommand{\V}{{\bf v}}
\newcommand{\U}{{\bf u}}
\renewcommand{\P}{{\bf p}}
\newcommand{\Kappa}{{\cal K}}
\newcommand{\fma}{{\small {\tt fma}}}

\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator{\card}{\mathrm{card}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\bbI}{\ensuremath{\mathbb{I}}}
\newcommand{\frS}{\ensuremath{\mathfrak{S}}}
\newcommand\ubar[1]{\underaccent{\bar}{#1}}

\begin{document}
\title[AD2024, Chicago, Sep 16-19, 2024]{{\bf An Efficient Local Optimizer-Tracking Solver for Differential-Algebriac Equations with Optimization Criteria}}
\author[]{Alexander Fleming\footnote{\scriptsize fleming@stce.rwth-aachen.de} \and Jens Deussen \and Uwe Naumann}
\institute[STCE]{\underline{S}oftware and \underline{T}ools for \underline{C}omputational \underline{E}ngineering\footnote{\scriptsize https://www.stce.rwth-aachen.de/} \vspace{1mm} \\ RWTH Aachen, Germany \vspace{5mm} \\ 
8th International Conference on Algorithmic Differentiation (AD2024)
}
		
\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}
	\frametitle{Contents}
	\tableofcontents
\end{frame}

\section{Motivation}

\begin{frame}
\frametitle{Motivation}
\vfill
	Consider the following differential equation with an embedded optimization problem (DAEO):
	\begin{equation}
		\label{eq:newtonian-daeo}
		\begin{aligned}
		\partial_t x(t) &= f(x, y) = \argmin_y h(x, y) \qquad 0<t<T \\
		h(x, y) &= \left(\frac{x-y}{3}\right)^2 + 3\cos(\frac{\pi}{4}y)\\
		x(0) &= x_0
		\end{aligned}
	\end{equation}
\vfill
How might we approach solving this equation numerically?
\vfill
\end{frame}

\begin{frame}
	\frametitle{Motivation}
	\begin{center}
		\includegraphics[width=0.9\columnwidth]{../gfx/minima_shifting.pdf}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Motivation}
	The circular dependence of $x(t)$ on the solution to the minimization problem $y(t)$ poses an issue. We could try separating the two problems, and solving the optimization problem between each time step:
	\vfill
	\begin{equation}
		\begin{aligned}
			\label{eq:bad-time-stepping}
			y^\star(t) &= \argmin_y h(x^k(t), y)\\
			x^{k+1} &= x^{k} + \frac{\Delta t}{2}\left(f(x^k, y^\star) + f(x^{k+1}, y^\star)\right)
		\end{aligned}
	\end{equation}
	\vfill
	This approach has two problems:
	\begin{itemize}
		\item Global optimization is, especially for larger problems, is very expensive.
		\item What happens when the global optimizer $y^\star$ changes between two times $t^{k}, t^{k+1}$?
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Motivation}
	\vfill
	Without some way to detect a global optimizer change during a time step, any integration method we choose for $x(t)$ will have its order of convergence \textit{reduced to 1}!
\end{frame}

\section{Global Optimization with Interval Arithmetic}

\subsection{Interval Arithmetic}
\begin{frame}
	\frametitle{Interval Arithmetic}
	In order to solve this problem, we'll first need a global optimizer that can find \textit{all} local optimizers of $h(x, y)$.
	\vfill
	\begin{block}{Fundamental Theorem of Interval Arithmetic \supercite{hickeyIntervalArithmeticPrinciples2001}} 
		An interval evaluation of a function $\left[y\right] = \left[\ubar{y},\bar{y}\right] = f(\left[\ubar x, \bar x\right])=f([x])$ must yield an interval that contains all pointwise evaluations $f(x)\,\forall\,x\in\left[\ubar x, \bar x\right]$.
	\end{block}
	In the world of floating-point numbers, \textbf{IEEE 1788-2015} specifies basic interval arithmetic (IA). The Boost interval library\supercite{melquiondBoostIntervalLibrary2022} follows this standard.
	\vfill
	\vfill
	If we had access to \textit{interval gradients}, this process would be much easier...
	\vfill
\end{frame}

\begin{frame}
	\frametitle{Interval Automatic Differentiation}
	Consider some function $f$, and its tangent- and adjoint-mode evaluations:
	\begin{gather*}
		y = f(x) \colon \R^n\mapsto\R^m\\
		y^{(1)} = \nabla_xf(x)\cdot x^{(1)}\\
		x_{(1)} = y_{(1)}\cdot\nabla_xf(x)
	\end{gather*}
	With a suitable AD tool (perhaps \texttt{dco/c++}\supercite{leppkesDerivativeCodeOverloading2016}), we could substitute intervals directly into tangent- and adjoint-mode AD.
	\vfill
	\begin{minipage}{0.42\columnwidth}
		An interval evaluation of the tangent of $f$ must yield a correct interval for $y^{(1)}$:
		\begin{equation*}
			[y^{(1)}] = \nabla_xf([x])\cdot x^{(1)}
		\end{equation*}
	\end{minipage}
	\hfill
	\begin{minipage}{0.42\columnwidth}
	An interval evaluation of the adjoint of $f$ must yield a correct interval for $x_{(1)}$:
	\begin{equation*}
		[x_{(1)}] = y_{(1)}\cdot\nabla_xf([x])
	\end{equation*}
	\end{minipage}
	\vfill
\end{frame}

\begin{frame}
	\frametitle{Optimality Conditions with Intervals}
	Testing if an interval $[\ubar x, \bar x]$ contains a critical point is quite straightforward.
	\begin{block}{First-Order Optimality with Intervals}
		The interval $[x]$ contains a critical point if $0 \in [\nabla_x f_i]$.
	\end{block}
	\vfill
	Second-order optimality is a bit more difficult. We can compute the interval Hessian $[H_xf]$ via AD exactly as we would compute the regular Hessian.
	\begin{block}{Second-Order Optimality with Intervals}
		The interval $[x]$ contains exactly one minimum of $f$ if, for every matrix $A\in\left[H_xf([x])\right]$, $A$ is positive definite.
	\end{block}
\end{frame}

\subsection{An Optimization Algorithm}
\begin{frame}
	\frametitle{An Optimization Algorithm}
	\begin{block}{Branch-and-Act with Interval Arithmetic}
		Process the list of intervals to search for optimizers $\frS = \left\{[y]\right\}$ according to the following rules:
		\begin{enumerate} 
			\item Take the first item $[y]$ from $\frS$.
			\item \textbf{Gradient Test:} Evaluate the interval gradient $\partial_y h(x, [y])$. If the result interval does not contain $0$, $[y]$ contains no optimizers and can be discarded.
			\item \textbf{Hessian Test:} Test the interval Hessian $\partial^2_y h(x, [y])$ for positive definite-ness.
			\begin{enumerate}
				\item If the interval Hessian is negative definite, $h$ is concave down over the interval $[y]$, and $[y]$ can be discarded.
				\item If the interval Hessian is positive definite, $h$ is concave up over the interval $[y]$, and $[y]$ can be narrowed by any appropriate local optimization method.
			\end{enumerate}
			\item \textbf{Branch:} If the interval Hessian is neither positive- nor negative definite, decompose the interval $[y]$ and append the results to $\frS$.
			\item Repeat for all remaining items in $\frS$.
		\end{enumerate}
	\end{block}
\end{frame}
\section{Integrating a DAEO}
\subsection{Event Detection}
\begin{frame}
	\frametitle{Events}
	The reason that the initial integration strategy fails is because of "events".
	\vfill
	An event occurs at some time $\tau \geq t_0$, when there is a $y^i(\tau)\neq y^\star(\tau)\in\left\{y_k(\tau)\right\}$ such that
	\begin{equation}
		h(x(\tau), y^\star(\tau)) = h(x(\tau), y^i(\tau))
	\end{equation}
	Two scenarios can lead to an event:
	\begin{itemize}
		\item The global optimizer shifts from $y^i(t)$ to $y^j(t)$ as the system evolves.
		\item The global optimizer is both $y^i(\tau)$ and $y^j(\tau)$ \textit{only} at $t=\tau$.
	\end{itemize}
	\vfill
\end{frame}

\begin{frame}
	\frametitle{Events}
	We can test if a time step from $t_k$ to $t_{k+1}$ contained an event between any two local optimizers $y^i$ and $y^j$ by testing for a sign change in the \textit{event function}
	\begin{equation}
		H(x^k, y^i, y^j) = h(x^k, y^i) - h(x^k, y^j)
	\end{equation}
	\vfill
	\vfill
	In time steps where this situation occurs, a root-finding procedure can find the time $\tau$ where $H(x^k, y^i, y^j) = 0$, and the time stepping procedure for \eqref{eq:newtonian-daeo} can step from $t_k$ to $\tau$ to $t_{k+1}$.
	\vfill
\end{frame}

\subsection{Local Optimizer Tracking}

\begin{frame}
	\frametitle{Local Optimizer Tracking}
Estimating local optimizer drift:
	\begin{equation*}
		\label{eq:local-tracking-guess}
		\begin{aligned}
			0&=\partial_{y}h(x, y^k)\\
			0&=d_x\partial_yh(x, y^k)\\
			0&=\partial^2_{yy}h(x, y^k)\cdot\partial_xy^k + \partial^2_{xy}h(x,y^k)\\
			\partial_xy^k &= -\left(\partial^2_{yy}h(x,y^k)\right)^{-1}\partial^2_{xy}h(x, y^k)
		\end{aligned}
	\end{equation*}
	Time stepping:
	\begin{equation}
		\label{eq:integrator-with-tracking}
		\begin{aligned}
			0 &= x^k - x^{k+1} + \frac{\Delta t}{2}\left(f(x^k, 	y^{\star,n})+f(x^{k+1}, y^{\star,k+1})\right)\\
			0 &= \partial_{y^k_i}h\left(x^{k+1}, y^{k+1}_i\right)
		\end{aligned}
	\end{equation}
\end{frame}

\section{Performance Testing}
\section{Context}
\begin{frame}
	\frametitle{Context \& Related Work}
\end{frame}
\section{Conclusion and Outlook}
\begin{frame}
	\frametitle{Conclusions}
	
\end{frame}

\begin{frame}
	\frametitle{Outlook}
	I don't want to work on this anymore.
\end{frame}

\section{References}
\begin{frame}
	\frametitle{References}
	\printbibliography
\end{frame}



\end{document}
